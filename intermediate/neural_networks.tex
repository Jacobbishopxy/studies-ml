% @file:		neural_networds.tex
% @author:	Jacob Xie
% @date:		2023/03/18 11:57:29 Saturday
% @brief:

\documentclass[../studies-ml.tex]{subfiles}

\begin{document}

\subsection{神经元模型}

\begin{center}
  \begin{tabular}{ |p{3cm}||p{4cm}|p{6cm}|  }
    \hline
    \multicolumn{3}{|c|}{\textbf{神经元模型}}                                               \\
    \hline
    名称   & 英文                  & 描述                                                    \\
    \hline
    神经网络 & neural networks     & 由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应 \\
    \hline
    神经元  & neuron              & 神经网络中最基本的成分                                           \\
    \hline
    阈值   & threshold           & 公式中记作 $\theta$                                        \\
    \hline
    连接   & connection          &                                                       \\
    \hline
    激活函数 & activation function & 处理以产生神经元的输出                                           \\
    \hline
    挤压函数 & squashing function  &                                                       \\
    \hline
  \end{tabular}
\end{center}

\subsection{感知机与多层网络}

感知机（Perceptron）由两层神经元组成。如图示，输入层接受外界输入信号后传递给输出层，输出层是 M-P 神经元，也称阈值逻辑单元
（threshold logic unit），其中 $y = f(\sum_i w_i x_i - \theta)$，而 $f$ 为激活函数。

\begin{center}
  \begin{tikzpicture}[shorten >=1pt,->]
    \tikzstyle{unit}=[draw,shape=circle,minimum size=1]

    \node[unit](p) at (0,1){};
    \node[unit](x1) at (-0.8,0){};
    \node[unit](x2) at (0.8,0){};
    \node[below] at (x1.south){$x_1$};
    \node[below] at (x2.south){$x_2$};
    \node[below,left=0.35,yshift=-8] at (p.south){$w_1$};
    \node[below,right=0.35,yshift=-8] at (p.south){$w_2$};

    \draw [->] (x1) -- (p);
    \draw [->] (x2) -- (p);
    \draw (p) -- ++(0,1) node[xshift=-10]{$y$};

    \node [align=center, right=1.6] at (p) {\footnotesize{输出层}};
    \node [align=center, right=0.8] at (x2) {\footnotesize{输入层}};
  \end{tikzpicture}
\end{center}

一般而言，给定训练数据集，权重 $w_i\, (i=1,2,\dots,n)$ 以及阈值 $\theta$ 可通过学习得到。而阈值 $\theta$ 可视为一个固定输入
为 -1.0 的哑结点（dummy node）所对应的链接权重 $w_{n+1}$，因此权重和阈值的学习就可以统一为权重的学习。感知机的学习规则非常简单，
对训练样例 $(\pmb{x},y)$ 而言，如果当前感知机的输出位 $\hat{y}$，则感知机权重将这样调整：

\begin{equation}
  w_i \leftarrow w_i + \Delta w_i
\end{equation}

\begin{equation}
  \Delta w_i = \eta (y - \hat{y}) x_i
\end{equation}

其中 $\eta \in (0,1)$ 称为学习率（learning rate）。公式 5.2 为感知机学习算法中的参数更新式。

\newpage

\begin{enumerate}[I]
  \item \textbf{感知机模型}

        感知机模型的式可表示为：

        \begin{align*}
          \begin{split}
            y & = f \Biggl( \sum_{i=1}^{n} w_i x_i - \theta \Biggr) \\
            & = f(\pmb{w}^T \pmb{x} - \theta)
          \end{split}
        \end{align*}

        其中，$x \in \mathbb{R}^n$ 即样本的特征向量，是感知机模型的输入；$\pmb{w}, \theta$ 是感知机模型的参数，权重 $w \in \mathbb{R}^n$，
        $\theta$ 为阈值。假定 $f$ 为阶跃函数，那么感知机模型的式可以表示为：

        \begin{equation*}
          y = \varepsilon (\pmb{w}^T \pmb{x} - \theta) = \begin{cases}
            1,\quad \pmb{w}^T \pmb{x} - \theta \ge 0; \\
            0,\quad \pmb{w}^T \pmb{x} - \theta < 0.
          \end{cases}
        \end{equation*}

        由于 $n$ 维空间中的超平面方程为
        \[
          w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b = \pmb{w}^T \pmb{x} + b = 0
        \]

        因此感知机模型式中的 $\pmb{w}^T \pmb{x} - \theta$ 可视为 $n$ 维空间中的一个超平面，将 $n$ 维空间划分为 $\pmb{w}^T \pmb{x} - \theta \ge 0$
        与 $\pmb{w}^T \pmb{x} - \theta < 0$ 的两个子空间（\textit{试想一下三维空间下的一个平面将空间切分为两部分}）。
        那么落在前一个子空间的样本对应的模型输出值为 1，而后者为 0，如此实现了分类功能。

  \item \textbf{学习策略}

        给定一个线性可分的数据集 $T$，感知机的学习目标是求得能对数据集 $T$ 中的正负样本完全正确划分的分离超平面
        \[ \pmb{w}^T \pmb{x} - \theta = 0 \]

        假设此时误分类样本集合为 $M \subset T$，对任意一个误分类样本 $(\pmb{x},y) \in M$ 而言，当 $\pmb{w}^T \pmb{x} - \theta \ge 0$ 时，
        模型输出值为 $\hat{y} = 1$，样本真实标记为 $y = 0$；反之亦然。综上，以下式恒成立：
        \[ (\hat{y} - y)(\pmb{w}^T \pmb{x} - \theta) \ge 0 \]

        因此对于给定数据集 $T$，其损失函数可以定义为
        \[ L(\pmb{w}, \theta) = \sum_{\pmb{x} \in M} (\hat{y} - y)(\pmb{w}^T \pmb{x} - \theta) \]

        非负之和显然非负，因此损失函数为非负。当没有误分类点时，损失函数的值为 $0$；误分类点越少，误分类点离超平面越近，损失函数值就越小。
        因此对于给定的数据集 $T$，损失函数 $L(\pmb{w}, \theta)$ 是关于 $\pmb{w}, \theta$ 的连续可导函数（注意是关于 $\pmb{w}, \theta$
        的可导，意味着之后将要对其进行梯度下降算法，即对其使用导数计算）。

        连续：
        \[
          \lim_{x \to x_0} f(x) = f(x_0)
        \]

        可导：
        \[
          f'(x_0) = \lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} =
          \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x}
        \]

  \item \textbf{学习算法}

        感知机模型的学习问题可以转化为求解损失函数的最优化问题。给定数据集
        \[ T = \{(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),\cdots,(\pmb{x}_N,y_N),\} \]
        其中 $\pmb{x}_i \in \mathbb{R}, y_i \in \{0,1\}$，求参数 $\pmb{w}, \theta$ 使得损失函数最小化：
        \[
          \min_{\pmb{w},\theta} L(\pmb{w},\theta) =
          \min_{\pmb{w},\theta} \sum_{\pmb{x}_i \in M} (\hat{y}_i - y_i)(\pmb{w}^T \pmb{x} - \theta)
        \]

        其中 $M \subset T$ 为误分类样本集合。若将阈值 $\theta$ 视为一个固定输入为 $-1$ 的 “哑结点”（前文有提到），即：
        \[ -\theta = -1 \cdot w_{n+1} = x_{n+1} \cdot w_{n+1} \]

        那么 $\pmb{w}^T \pmb{x} - \theta$ 可简化为

        \begin{align*}
          \begin{split}
            \pmb{w}^T \pmb{x} - \theta & = \sum_{j=1}^{n} w_j x_j + x_{n+1} \cdot w_{n+1} \\
            &= \sum_{j=1}^{n+1} w_j x_j \\
            &= \pmb{w}^T \pmb{x_i}
          \end{split}
        \end{align*}

        其中 $\pmb{x_i} \in \mathbb{R}^{n+1},\, \pmb{w} \in \mathbb{R}^{n+1}$，有此可将最小化问题进一步简化：
        \[
          \min_{\pmb{w}} L(\pmb{w}) = \min_{\pmb{w}} \sum_{\pmb{x_i} \in M} (\hat{y}_i - y_i) \pmb{w}^T \pmb{x_i}
        \]

        假设误分类样本集合 $M$ 固定，那么可以求得损失函数 $L(\pmb{w})$ 的梯度
        \[ \nabla_{\pmb{w}} L(\pmb{w}) = \sum_{\pmb{x_i} \in M} (\hat{y}_i - y_i) \pmb{x_i} \]

        感知机的学习算法具体采用的是\textbf{随机梯度下降法}，即在最小化的过程中，不是一次使 $M$ 中所有误分类点的梯度下降，
        而是一次随机选取一个误分类点，并使其梯度下降。所以权重 $\pmb{w}$ 的更新式为
        \begin{gather*}
          \pmb{w} \leftarrow \pmb{w} + \Delta \pmb{w}, \\
          \Delta \pmb{w} = -\eta (\hat{y}_i - y_i) \pmb{w} = \eta (y_i - \hat{y}_i) \pmb{w}
        \end{gather*}

        即 $\pmb{w}$ 中的某个分量 $w_i$ 的更新式即式 5.2。

        备注：这里随机的意义在于每次调整的权重 $w$ 不会对某个/些样本点产生依赖，即常说的“过拟合”。
\end{enumerate}

\bigbreak
\textbf{补充}

\begin{enumerate}[I]
  \item \textbf{梯度下降法（gradient descent）}是一种一阶（first-order）优化方法，其被用于求解无约束优化问题。
        假设一个无约束优化 $min_{\pmb{x}} f(\pmb{x})$，其中 $f(\pmb{x})$ 为\textbf{连续可微函数}。
        若能构成一个序列 $\pmb{x}^0,\pmb{x}^1,\pmb{x}^2,\dots$ 满足：
        \begin{equation*}
          f(\pmb{x}^{t+1}) < f(\pmb{x}^{t}),\quad t = 0,1,2,\dots
        \end{equation*}
        则不断执行该过程即可收敛到局部极小点（注：收敛级数 series converges）。为了满足以上式，根据泰勒展开有
        \begin{equation*}
          f(\pmb{x} + \Delta\pmb{x}) \simeq f(\pmb{x}) + \Delta \pmb{x}^T \nabla f(\pmb{x})
        \end{equation*}
        因此，为了满足 $f(\pmb{x} + \Delta\pmb{x}) < f(\pmb{x})$，可选择
        \begin{equation*}
          \Delta \pmb{x} = -\gamma \nabla f(\pmb{x})
        \end{equation*}
        其中步长 $\gamma$ 为一个小常数。这就是梯度下降法。

  \item Sigmoid 函数作为上述的连续可微函数（激活函数）
        \begin{center}
          \begin{tikzpicture}[declare function={sigma(\x)=1/(1+exp(-\x));
                  sigmap(\x)=sigma(\x)*(1-sigma(\x));}]
            \begin{axis}%
              [
                grid=major,
                xmin=-6,
                xmax=6,
                axis x line=bottom,
                ytick={0,.5,1},
                ymax=1,
                axis y line=middle,
                samples=100,
                domain=-6:6,
                legend style={at={(1,0.9)}}
              ]
              \addplot[blue,mark=none]   (x,{sigma(x)});
              \addplot[red,dotted,mark=none]   (x,{sigmap(x)});
              \legend{$\sigma(x)$,$\sigma'(x)$}
            \end{axis}
          \end{tikzpicture}
        \end{center}

  \item $\nabla$ 在数学中用于表示向量的微分计算。当 $\nabla$ 应用在一维域的函数上时，即为标准的函数求导；
        当 $\nabla$ 应用在一个场上（也就是多维域），它可能代表着如下三种含义中的一个：
        \begin{enumerate}[i]
          \item 梯度（gradient）：$f = \nabla f$
          \item 向量的发散度（divergence）：$\textrm{div}\, \overrightarrow{v} = \nabla \cdot \overrightarrow{v}$
          \item 向量的旋度（curl）：$\textrm{curl}\, \overrightarrow{v} = \nabla \times \overrightarrow{v}$
        \end{enumerate}
\end{enumerate}

本章其余名词：

\begin{center}
  \begin{tabular}{ |p{4cm}||p{7cm}|  }
    \hline
    \multicolumn{2}{|c|}{\textbf{神经元模型}}               \\
    \hline
    名称       & 英文                                      \\
    \hline
    功能神经元    & functional neuron                       \\
    \hline
    线性可分     & linearly separable                      \\
    \hline
    收敛       & converge                                \\
    \hline
    震荡       & fluctuation                             \\
    \hline
    隐层或隐含层   & hidden layer                            \\
    \hline
    多层前馈神经网络 & multi-layer feedforward neural networks \\
    \hline
    连接权      & connection weight                       \\
    \hline
  \end{tabular}
\end{center}

\newpage

\subsection{误差逆传播算法}

误差逆传播算法（error BackPropagation，简称 BP）。

给定训练集 $D = \{(\pmb{x}_1, \pmb{y}_1), (\pmb{x}_2, \pmb{y}_2),\dots,(\pmb{x}_m, \pmb{y}_m)\},\,
  \pmb{x}_i \in \mathbb{R}^d,\, \pmb{y}_i \in \mathbb{R}^l$，即 input 为 $d$ 维，output $l$ 维。如图：

\begin{center}
  \tikzset{%
    every neuron/.style={
        circle,
        draw,
        minimum size=0.7cm
      },
    neuron missing/.style={
        draw=none,
        scale=2,
        text width=0.444cm,
        execute at begin node=\color{black}$\hdots$
      },
  }

  \begin{tikzpicture}[x=1cm, y=1.5cm, >=stealth]

    % ================================================================================================
    % 神经元
    % ================================================================================================
    \foreach \m [count=\i] in {1,missing,2,missing,3}
    \node [every neuron/.try, neuron \m/.try ] (output-\m) at (\i+2.22,4) {};

    \foreach \m [count=\i] in {1,2,missing,3,missing,4}
    \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (\i*1.5,2) {};

    \foreach \m [count=\i] in {1,missing,2,missing,3}
    \node [every neuron/.try, neuron \m/.try] (input-\m) at (\i+2.22,0) {};

    % ================================================================================================
    % 神经元标记
    % ================================================================================================
    \foreach \m [count=\i] in {1,j,l}
    \node [above] at (output-\i.north) {$y_\m$};

    \foreach \m [count=\i] in {1,2,h,q}
    \node [above=0.1,right] at (hidden-\i.east) {$b_\m$};

    \foreach \m [count=\i] in {1,i,d}
    \node [below] at (input-\i.south) {$x_\m$};


    % ================================================================================================
    % 注解
    % ================================================================================================
    \draw [red,dashed,->] (output-2) -- ++(0.8,-0.35) -- ++(3.4,0)
    node [below=0.33,right] {\shortstack{第 $j$ 个输出神经元的输入 \\ $\beta_j = \sum\limits_{h=1}^{q} w_{hj}b_h$}};

    \draw [red,dashed,->] (hidden-3) -- ++(0.8,-0.45) -- ++(2.6,0)
    node [below=0.33,right] {\shortstack{第 $h$ 个隐层神经元的输入 \\ $\alpha_j = \sum\limits_{i=1}^{d} v_{ih}x_i$}};

    % ================================================================================================
    % 连接
    % ================================================================================================
    \foreach \i in {1,...,4}
    \foreach \j in {1,3}
    \draw [-] (hidden-\i.north) -- (output-\j.south);

    \foreach \i in {1,...,4}
    \draw [red,-] (hidden-\i.north) -- (output-2.south);

    \foreach \i in {1,...,3}
    \foreach \j in {1,2,4}
    \draw [-] (input-\i.north) -- (hidden-\j.south);

    \foreach \i in {1,...,3}
    \draw [red,-] (input-\i.north) -- (hidden-3.south);

    % ================================================================================================
    % 标记
    % ================================================================================================
    \foreach \m [count=\i] in {1,2,h,q}
    \node [yshift=10,fill=white,text=red,scale=0.8] at (hidden-\i.north) {$w_{\m j}$};

    \foreach \m [count=\i] in {1,i,d}
    \node [yshift=10,fill=white,text=red,scale=0.8] at (input-\i.north) {$v_{\m j}$};

    % ================================================================================================
    % 注解
    % ================================================================================================
    \foreach \l [count=\i from 0] in {输入, 隐, 输出}
    \node [align=center, left] at (0,\i*2) {\l 层};

  \end{tikzpicture}
\end{center}

如图所示，该神经网络是一个拥有 $d$ 个输入神经元，$l$ 个输出神经元，$q$ 个隐层神经元的多层前馈网络结构。
\bigbreak

其中输出层第 $j$ 个神经元的\textcolor{Cerulean}{\textbf{阈值}}用 \textcolor{Cerulean}{$\theta_j$} 表示，
隐层第 $h$ 个神经元的\textcolor{Cerulean}{\textbf{阈值}}用 \textcolor{Cerulean}{$\gamma_h$} 表示；
输入层第 $i$ 个神经元与隐层第 $h$ 个神经元之间的\textcolor{BurntOrange}{\textbf{连接权}}为 \textcolor{BurntOrange}{$v_{ih}$}，
隐层第 $h$ 个神经元与输出层第 $j$个神经元之间的\textcolor{BurntOrange}{\textbf{连接权}}为 \textcolor{BurntOrange}{$w_{hj}$}。
\bigbreak

其中隐层的第 $h$ 个神经元接收到的\textcolor{Plum}{\textbf{输入}}
为 \textcolor{Plum}{$\alpha_h = \sum_{i=1}^{d} v_{ih} x_i$}（图中输入层与隐层之间的三条红线所示）；
输出层的第 $j$ 个神经元接收到的\textcolor{Plum}{\textbf{输入}}
为 \textcolor{Plum}{$\beta_j = \sum_{h=1}^{q} w_{hj} b_h$}（图中隐层与输出层之间的四条红线所示），
其中 \textcolor{ForestGreen}{$b_h$} 为隐层第 $h$ 个神经元的\textcolor{ForestGreen}{\textbf{输出}}。
\bigbreak

对训练例 $(\pmb{x}_k, \pmb{y}_k)$ 假设神经网络的输出为 $\hat{\pmb{y}}_k = (\hat{y}_1^k,\hat{y}_2^k,\dots,\hat{y}_l^k,)$，即
\begin{equation}
  \hat{y}_j^k = f(\beta_j - \theta_j)
\end{equation}

这里的真实值 $y_j^k = (y_1^k,y_2^k,\dots,y_l^k)^2$，那么对于某第 i 个输出层的神经元而言，其均方误差即
\[
  \frac{(\hat{y}_i^k - y_i^k)}{2}
\]
那么将所有神经元加总，就有了网络在 $(\pmb{x}_k, \pmb{y}_k)$ 上的均方误差为：
\begin{equation}
  E_k = \frac{1}{2} \sum_{j=1}^{l} (\hat{y}_j^k - y_j^k)^2
\end{equation}

另外上图的网络中有 $(d + l + l) q + l$ 个参数需确定：输入层到隐层的 $d \times q$ 个权值、隐层到输出层的 $q \times l$ 个权值、
$q$ 个隐层神经元的阈值、$l$ 个输出层神经元的阈值。BP 是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计，
即与式 5.1 类似，任意参数 $v$ 的更新估计式为
\begin{equation}
  v \leftarrow v + \Delta v.
\end{equation}

BP 算法基于 \textbf{梯度下降（gradient descent）} 策略，以目标的\textit{负}梯度方向\textit{对参数进行调整}。
对式 5.4 的误差 $E_k$，给定学习率 $\eta$，有
\begin{equation}
  \Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}}
\end{equation}

这里提到的负梯度，是利用了导数趋近于零时可以得到最小值的特性，去求得均方误差 $E_k$ 的最小值。回顾上文中的感知机学习算法：
\begin{aquote}
  ...随机选取一个误分类点并使其梯度下降，所以权重 $\pmb{w}$ 的更新式为
  \begin{gather*}
    \pmb{w} \leftarrow \pmb{w} + \Delta \pmb{w}, \\
    \Delta \pmb{w} = -\eta (\hat{y}_i - y_i) \pmb{w} = \eta (y_i - \hat{y}_i) \pmb{w}
  \end{gather*}
\end{aquote}

式 5.6 与感知机的最小化损失函数的不同之处在于：感知机是有两层神经元构成的，且输出层只有一个神经元，因此只需要考虑一层的权重变化，
即 $(\hat{y}_i - y_i) \pmb{w}$；而对于由若干个感知机所构成的神经网络而言，所有权重的变化则变为了
$\frac{\partial E_k}{\partial w_{hj}}$。

那么对于多层的神经网络，式 5.4 仅表现出了最后一层隐层与输出层的权重变化，那么就有了疑问：其它层之间的权重好像并不会被式 5.6 所改变？
我们接着往下看，注意到 $w_hj$ 先影响到第 $j$ 个输出层神经元的输入值 $\beta_j$。根据图示其为隐层所有神经元与其本身的表达式，即
\[ \beta_j = \sum_{h=1}^{q} w_{hj} b_h \]
接着 $w_{hj}$ 再影响到其他输出值 $\hat{y}_j^k$，最后再影响到 $E_k$，那么根据求导的链式法则有：
\begin{equation}
  \frac{\partial E_k}{\partial w_{hj}} =
  \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
  \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot
  \frac{\partial \beta_j}{\partial w_{hj}}
\end{equation}

根据 $\beta_j$ 的定义，有：
\begin{equation}
  \begin{split}
    \frac{\partial \beta_j}{\partial w_{hj}} & = \frac{\partial (\sum_{h=1}^{q} w_{hj} b_h) }{\partial w_{hj}} \\
    & = \frac{\partial (w_{1j}b_1 + w_{2j}b_2 + \cdots + w_{hj}b_h + \cdots + w_{qj}b_q)}{\partial w_{hj}} \\
    & = 0 + 0 + \cdots + b_h + \cdots + 0 \\
    & = b_h
  \end{split}
\end{equation}

而又根据 Sigmoid 函数的一个很好的性质：
\begin{equation}
  f'(x) = f(x)(1 - f(x))
\end{equation}

根据式 5.4 与式 5.3 有：
\begin{equation}
  \begin{split}
    g_j & = -\frac{\partial E_k}{\partial \hat{y}_j^k} \cdot \frac{\partial \hat{y}_j^k}{\partial \beta_j} \\
    & = -(\hat{y}_j^k - y_j^k)f'(\beta_j - \theta_j) \\
    & = \hat{y}_j^k (1 - \hat{y}_j^k)(y_j^k - \hat{y}_j^k)
  \end{split}
\end{equation}

将式 5.10 与式 5.8 代入式 5.7，再代入式 5.6，就得到了 BP 算法中关于 $w_{hj}$ 的更新公式：
\begin{equation}
  \begin{split}
    \Delta w_{hj} & = -\eta \frac{\partial E_k}{\partial w_{hj}} \\
    & = -\eta \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot
    \frac{\partial \beta_j}{\partial w_{hj}} \\
    & = \eta \hat{y}_j^k (1 - \hat{y}_j^k)(y_j^k - \hat{y}_j^k) \cdot b_h \\
    & = \eta g_j b_h
  \end{split}
\end{equation}

类似可得：
\begin{equation}
  \Delta \theta_j = -\eta g_j
\end{equation}
\begin{equation}
  \Delta v_{ih} = \eta e_h x_i
\end{equation}
\begin{equation}
  \Delta \gamma_h = -\eta e_h
\end{equation}

式 5.13 与 5.14 中

\begin{equation}
  \begin{split}
    e_n & = -\frac{\partial E_k}{\partial b_h} \cdot \frac{\partial b_h}{\partial \alpha_h} \\
    & = -\sum_{j=1}^{i} \frac{\partial E_k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} f'(\alpha_h - \gamma_h) \\
    & = \sum_{j=1}^{l} w_{hj} g_j f'(\alpha_h - \gamma_h) \\
    & = b_h (1 - b_h) \sum_{j=1}^{l} w_{hj} g_j
  \end{split}
\end{equation}

\bigbreak
\textbf{式 5.12 解析}

因为：

\[
  \Delta \theta_j = -\eta \frac{\partial E_k}{\partial \theta_j}
\]

同时有：

\begin{align*}
  \begin{split}
    \frac{\partial E_k}{\partial \theta_j} & = \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial \hat{y}_j^k}{\partial \theta_j} \\
    & = \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial [f(\beta_j - \theta_j)]}{\partial \theta_j} \\
    & = \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    f'(\beta_j - \theta_j) \times (-1) \\
    & = \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    f(\beta_j - \theta_j) \times [1 - f(\beta_j - \theta_j)] \times (-1) \\
    & = \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \hat{y}_j^k (1 - \hat{y}_j^k) \times (-1) \\
    & = \frac{\partial [\frac{1}{2} \sum_{j=1}^{l} (\hat{y}_j^k - y_j^k)^2]}{\partial \hat{y}_j^k} \cdot
    \hat{y}_j^k (1 - \hat{y}_j^k) \times (-1) \\
    & = \frac{1}{2} \times 2(\hat{y}_j^k - y_j^k) \times 1 \cdot \hat{y}_j^k (1 - \hat{y}_j^k) \times (-1) \\
    & = (\hat{y}_j^k - y_j^k) \hat{y}_j^k (1 - \hat{y}_j^k) \\
    & = g_j
  \end{split}
\end{align*}

所以：

\[
  \Delta \theta_j = -\eta g_j = -\eta \frac{\partial E_k}{\partial \theta_j}
\]

\textbf{式 5.13 解析}

因为：

\[
  \Delta v_{ih} = -\eta \frac{\partial E_k}{\partial v_{ih}}
\]

同时有：

\begin{align*}
  \begin{split}
    \frac{\partial E_k}{\partial v_{ih}} & = \sum_{j=1}^{l} \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} \cdot
    \frac{\partial b_h}{\partial \alpha_h} \cdot \frac{\partial \alpha_h}{\partial v_ih} \\
    & = \sum_{j=1}^{l} \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} \cdot
    \frac{\partial b_h}{\partial \alpha_h} \cdot x_i \\
    & = \sum_{j=1}^{l} \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} \cdot
    f'(\alpha_h - \gamma_h) \cdot x_i \\
    & = \sum_{j=1}^{l} \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot w_{hj} \cdot f'(\alpha_h - \gamma_h) \cdot x_i \\
    & = \sum_{j=1}^{l} (-g_j) \cdot w_{hj} \cdot f'(\alpha_h - \gamma_h) \cdot x_i \\
    & = - f'(\alpha_h - \gamma_h) \cdot \sum_{j=1}^{l} (g_j) \cdot w_{hj} \cdot x_i \\
    & = - b_h (1 - b_h) \cdot \sum_{j=1}^{l} (g_j) \cdot w_{hj} \cdot x_i \\
    & = - e_h \cdot x_i
  \end{split}
\end{align*}

所以：

\[
  \Delta v_{ih} = -\eta \frac{\partial E_k}{\partial v_{ih}} = \eta e_h x_i
\]

\textbf{式 5.14 解析}

因为：

\[
  \Delta \gamma_h = -\eta \frac{\partial E_k}{\partial \gamma_h}
\]

同时有：

\begin{align*}
  \begin{split}
    \frac{\partial E_k}{\partial \gamma_h} & = \sum_{j=1}^{l} \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} \cdot
    \frac{\partial b_h}{\partial \gamma_h} \\
    & = \sum_{j=1}^{l} \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} \cdot
    f'(\alpha_h - \gamma_h) \cdot (-1) \\
    & = -\sum_{j=1}^{l} \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot w_{hj} \cdot f'(\alpha_h - \gamma_h) \\
    & = -\sum_{j=1}^{l} \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot
    \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot w_{hj} \cdot b_h (1 - b_h) \\
    & = \sum_{j=1}^{l} g_j \cdot w_{hj} \cdot b_h (1 - b_h) \\
    & = e_h
  \end{split}
\end{align*}

所以：

\[
  \Delta \gamma_h = -\eta \frac{\partial E_k}{\partial \gamma_h} = -\eta e_h
\]

\newpage
以下为 BP 算法的工作流程：

\begin{shaded}
  \textbf{输入}：

  训练集 $D = \{(\pmb{x}_k, \pmb{y_k})\}_{k=1}^m$;

  学习率 $\eta$.
  \bigskip

  \textbf{过程}：

  1: 在 $(0,1)$ 范围内随机初始化网络中所有连接权和阈值

  2: \textbf{repeat}

  3. \aidnt \textbf{for all} $(\pmb{x}_k, \pmb{y}_k) \in D$ \textbf{do}

  4. \aidnt[2] 根据当前参数和式 5.3 计算当前样本的输出 $\hat{\pmb{y}}_k$；
  \[ \hat{y}_j^k = f(\beta_j - \theta_j) \]

  5. \aidnt[2] 根据式 5.10 计算输出层神经元的梯度项 $g_j$；
  \[ g_j = \hat{y}_j^k (1 - \hat{y}_j^k) (y_j^k - \hat{y}_j^k) \]

  6. \aidnt[2] 根据式 5.15 计算隐层神经元的梯度项 $e_h$；
  \[ e_h = b_h (1 - b_h) \sum_{j=1}^{l} w_{hj} g_j \]

  7. \aidnt[2] 根据式 5.11 - 5.14 更新连接权 $w_{hj},\ v_{ih}$ 与阈值 $\theta_j,\ \gamma_h$
  \begin{equation*}
    \begin{cases}
      \Delta w_{hj} = \eta g_j b_h \\
      \Delta \theta_j = -\eta g_j  \\
      \Delta v_{ih} = \eta e_h x_i \\
      \Delta \gamma_h = -\eta e_h
    \end{cases}
  \end{equation*}

  8. \aidnt \textbf{end for}

  9. \textbf{until} 达到停止条件

  \textbf{输出}：连接权与阈值确定的多层前馈神经网络
\end{shaded}

需要注意，BP 算法的目标是要最小化训练集 $D$ 上的累积误差（$m$ 即训练集的样本个数）
\begin{equation}
  E = \frac{1}{m} \sum_{k=1}^{m} E_k
\end{equation}

但是上面介绍的“标准BP算法”每次仅针对一个训练样例更新连接权和阈值，意味着上述工作流程中的更新规则是基于单个 $E_k$ 推导而得。
如果类似推导出基于累积误差最小化的更新规则，那么就得到了\textbf{累积误差逆传播（accumulated error backpropagation）}
算法。

\bigbreak
那么来比较一下两种算法的差异：

\begin{center}
  \begin{tabular}{ |p{2cm}||p{5cm}|p{5cm}|  }
    \hline
    \multicolumn{3}{|c|}{\textbf{BP 算法}}                     \\
    \hline
    差异   & 标准 BP 算法                 & 累积 BP 算法               \\
    \hline
    参数更新 & 只针对单个样例                  & 直接针对累积误差最小化            \\
    \hline
    更新频率 & 非常频繁，且对不同样例进行更新可能会出现“抵消” & 很低，因为是读取整个训练集后才对参数进行更新 \\
    \hline
    迭代次数 & 多                        & 少                      \\
    \hline
    类比   & 随机梯度下降                   & 标准梯度下降                 \\
    \hline
  \end{tabular}
\end{center}

注：随机梯度下降（stochastic gradient descent）
在训练集非常大时，累积误差下降到一定程度后，进一步下降会非常缓慢，这时切换为标准 BP 可以更快获得较好的解。

两种策略来缓解 BP 网络的过拟合：
\begin{enumerate}
  \item \textbf{早停（early stopping）}：将数据分成训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差，
        若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。
  \item \textbf{正则化（regularization）}：在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权与阈值的平方和。
        仍然令 $E_k$ 表示第 $k$ 个训练样例上的误差，$w_i$ 表示连接权和阈值，则误差目标函数 5.16 变为
        \begin{equation}
          E = \lambda \frac{1}{m} \sum_{k=1}^{m} E_k + (1-\lambda) \sum_{i} w_i^2
        \end{equation}
        其中 $\lambda \in (0,1)$ 用于对经验误差与网络复杂度这两项进行折中，通常会使用交叉验证法来估计。
\end{enumerate}

注：引入正则化策略的神经网络与第 6 章的 SVM 非常相似。

\newpage
\textbf{补充}

BP 算法的计算流程。

\begin{itemize}
  \item $x$ 输入，即特征向量；
  \item $y$ 目标输出。对于分类问题而言，输出是一个类型概率的向量（例 $(0.1, 0.7, 0.2)$），而目标输出是某个特定的类型，
        由 one-hot/dummy variable 编码（例 $(0, 1, 0)$）；
  \item $C$ 损失函数或代价函数。对于分类问题而言，这通常是交叉熵 cross entropy（XC），而对回归问题而言则是平方错误损失
        squared error loss（SEL）；
  \item $L$ 神经网络的层数；
  \item $W^l = （w_{jk}^l$ 神经网络中 $l-1$ 层与 $l$ 层之间的权重，而 $w_{jk}^l$ 则是在 $l-1$ 层的第 $k$ 个节点
        与 $l$ 层第 $j$ 个节点之间的权重；
  \item $f^l$ 神经网络中 $l$ 层的激活函数。对于分类问题，二元分类的最后一层通常是对数函数，多种类分类则是 softmax 函数，
        而传统做法中，隐层的节点通常是 sigmoid 函数，今天更多样化了，线性整流函数 rectified 更为普遍（ramp，ReLU）。
\end{itemize}

% TODO

\newpage
\subsection{全局最小与局部最小}

WIP

\end{document}
